{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa7f108-e491-40c9-b1d5-a30df4fcf17b",
   "metadata": {},
   "source": [
    "# 1. Embed using ProtT5_XL_Uniref50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002893da-c9d7-4937-a47c-f65bbb5e5d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from transformers import T5EncoderModel, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca3e72f-373e-4a31-b912-48b8daf3c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.has_mps else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90f279-c1df-4ec1-bd13-d4cd0d4c55ce",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7aa782-b293-4dce-8ac4-578092216143",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_t5_xl_uniref50 were not used when initializing T5EncoderModel: ['decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.layer_norm.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.embed_tokens.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'lm_head.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb58c4-aa23-4f99-a4a7-1c031cf4d623",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f277d5fc-f4dc-4579-b283-be9b0b232aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111564, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                      allele              seq  count_duplicates    aff_nM_max  \\\n0  HLA-DPA1*01:03/DPB1*02:01  AAAAGWQTLSAALDA                 1   3769.980300   \n1  HLA-DPA1*01:03/DPB1*02:01  AAAGAEAGKATTEEQ                 1  50000.000000   \n2  HLA-DPA1*01:03/DPB1*02:01  AAASVPAADKFKTFE                 2   5528.934759   \n3  HLA-DPA1*01:03/DPB1*02:01  AAATAGTTVYGAFAA                 1   7154.820918   \n4  HLA-DPA1*01:03/DPB1*02:01  AAATATATAAVGAAT                 1  50000.000000   \n\n     aff_nM_min  aff_nM_perc_diff   aff_nM_mean  aff_log50k_mean      subset  \\\n0   3769.980300          0.000000   3769.980300         0.238910       train   \n1  50000.000000          0.000000  50000.000000         0.000000        test   \n2   5520.014000          0.161608   5524.474380         0.203593  train-test   \n3   7154.820918          0.000000   7154.820918         0.179693       train   \n4  50000.000000          0.000000  50000.000000         0.000000        test   \n\n              dataset  length allele_fam           alpha        beta  \\\n0      NetMHCIIpan4.0      15         DP  HLA-DPA1*01:03  DPB1*02:01   \n1      NetMHCIIpan4.0      15         DP  HLA-DPA1*01:03  DPB1*02:01   \n2  44k-NetMHCIIpan4.0      15         DP  HLA-DPA1*01:03  DPB1*02:01   \n3                 44k      15         DP  HLA-DPA1*01:03  DPB1*02:01   \n4      NetMHCIIpan4.0      15         DP  HLA-DPA1*01:03  DPB1*02:01   \n\n   cat_bin_500  cat_multi  a_encoded  \n0            1          2        0.0  \n1            1          3        0.0  \n2            1          3        0.0  \n3            1          3        0.0  \n4            1          3        0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>allele</th>\n      <th>seq</th>\n      <th>count_duplicates</th>\n      <th>aff_nM_max</th>\n      <th>aff_nM_min</th>\n      <th>aff_nM_perc_diff</th>\n      <th>aff_nM_mean</th>\n      <th>aff_log50k_mean</th>\n      <th>subset</th>\n      <th>dataset</th>\n      <th>length</th>\n      <th>allele_fam</th>\n      <th>alpha</th>\n      <th>beta</th>\n      <th>cat_bin_500</th>\n      <th>cat_multi</th>\n      <th>a_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>HLA-DPA1*01:03/DPB1*02:01</td>\n      <td>AAAAGWQTLSAALDA</td>\n      <td>1</td>\n      <td>3769.980300</td>\n      <td>3769.980300</td>\n      <td>0.000000</td>\n      <td>3769.980300</td>\n      <td>0.238910</td>\n      <td>train</td>\n      <td>NetMHCIIpan4.0</td>\n      <td>15</td>\n      <td>DP</td>\n      <td>HLA-DPA1*01:03</td>\n      <td>DPB1*02:01</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HLA-DPA1*01:03/DPB1*02:01</td>\n      <td>AAAGAEAGKATTEEQ</td>\n      <td>1</td>\n      <td>50000.000000</td>\n      <td>50000.000000</td>\n      <td>0.000000</td>\n      <td>50000.000000</td>\n      <td>0.000000</td>\n      <td>test</td>\n      <td>NetMHCIIpan4.0</td>\n      <td>15</td>\n      <td>DP</td>\n      <td>HLA-DPA1*01:03</td>\n      <td>DPB1*02:01</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HLA-DPA1*01:03/DPB1*02:01</td>\n      <td>AAASVPAADKFKTFE</td>\n      <td>2</td>\n      <td>5528.934759</td>\n      <td>5520.014000</td>\n      <td>0.161608</td>\n      <td>5524.474380</td>\n      <td>0.203593</td>\n      <td>train-test</td>\n      <td>44k-NetMHCIIpan4.0</td>\n      <td>15</td>\n      <td>DP</td>\n      <td>HLA-DPA1*01:03</td>\n      <td>DPB1*02:01</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HLA-DPA1*01:03/DPB1*02:01</td>\n      <td>AAATAGTTVYGAFAA</td>\n      <td>1</td>\n      <td>7154.820918</td>\n      <td>7154.820918</td>\n      <td>0.000000</td>\n      <td>7154.820918</td>\n      <td>0.179693</td>\n      <td>train</td>\n      <td>44k</td>\n      <td>15</td>\n      <td>DP</td>\n      <td>HLA-DPA1*01:03</td>\n      <td>DPB1*02:01</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HLA-DPA1*01:03/DPB1*02:01</td>\n      <td>AAATATATAAVGAAT</td>\n      <td>1</td>\n      <td>50000.000000</td>\n      <td>50000.000000</td>\n      <td>0.000000</td>\n      <td>50000.000000</td>\n      <td>0.000000</td>\n      <td>test</td>\n      <td>NetMHCIIpan4.0</td>\n      <td>15</td>\n      <td>DP</td>\n      <td>HLA-DPA1*01:03</td>\n      <td>DPB1*02:01</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/input_with_features.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ff3c0-3332-4190-b3b9-5aa7c368c8ec",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4db8009b-ee3e-4e6b-9ce0-a9fbb27fa9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from df to list\n",
    "data = [ (str(x), y) for x, y in zip(df.index, df.seq)]\n",
    "assert len(data) == df.shape[0] # double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d56f411e-4499-4295-b490-74c8879cc201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\"\n",
    "    clean data for ProtT5 embedding\n",
    "    params data: list of tuples (str seq_name, str single aa seq).\n",
    "    return: tuple of lists: (list of seq names, list of clean seqs)\n",
    "    \"\"\"\n",
    "    seq_names = []\n",
    "    seqs = []\n",
    "    \n",
    "    for entry in data:\n",
    "        # append seq name\n",
    "        seq_names.append(entry[0])\n",
    "        \n",
    "        # process seq by add space between aa\n",
    "        s = list(entry[1])\n",
    "        seqs.append(' '.join(s))\n",
    "        \n",
    "    return seq_names, seqs\n",
    "\n",
    "seq_names, seqs = clean_data(data)\n",
    "assert len(seqs) == df.shape[0] # double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c4b568b-f1ff-45da-b060-7b994f2798eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_token_length(seqs):\n",
    "    \"\"\"\n",
    "    Get max token length\n",
    "    seqs: list of strings. Each string is in format \"A B C\"\n",
    "    return int, max token length to use for tokenizer\n",
    "    \"\"\"\n",
    "    # find longest seq\n",
    "    longest_seq = [max(seqs, key = len)]\n",
    "\n",
    "    # encode longest seq\n",
    "    ids = tokenizer.batch_encode_plus(longest_seq, add_special_tokens=True, padding=True)\n",
    "\n",
    "    # max token length\n",
    "    max_token_length = len(ids['input_ids'][0])\n",
    "\n",
    "    return max_token_length\n",
    "\n",
    "max_token_length = get_max_token_length(seqs)\n",
    "max_token_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b52eff72-a4c1-444e-946d-5f0b8bc3f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set arbitrary batch size\n",
    "batch_size = 2048\n",
    "\n",
    "# batch data\n",
    "seqs_batched = np.array_split(seqs, len(seqs)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85abe502-a576-434e-9f87-6b5508e20f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seqs_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca36400-2e5d-4ad3-a773-c61d5364a0eb",
   "metadata": {},
   "source": [
    "# Activate GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96e08186-3241-4d98-9c90-8a10a0201991",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6d1a1-29d4-4638-8f56-28565370c75e",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "877e953b-98af-4836-afa2-91b12d8abb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to eval\n",
    "model = model.eval()\n",
    "\n",
    "# Initiate features\n",
    "features = []\n",
    "\n",
    "for batch in seqs_batched:\n",
    "    # tokenize\n",
    "    ids = tokenizer.batch_encode_plus(batch, add_special_tokens=True, padding='max_length', max_length=max_token_length)\n",
    "    \n",
    "    # copy input tensors to device\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "    \n",
    "    # predict\n",
    "    with torch.no_grad():\n",
    "        batch_result = model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "    # extract features\n",
    "    batch_features = batch_result['last_hidden_state'].cpu().numpy()\n",
    "    \n",
    "    features.append(batch_features)\n",
    "    \n",
    "# concatenate features\n",
    "features = np.concatenate(features, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aca3546-032e-4867-94be-e2b674f252a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check \n",
    "assert features.shape == (df.shape[0], max_token_length, 1024) \n",
    "# number of proteins * tokens * features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77551b51-4043-4bf9-a853-0a4a0f242515",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a66f310-1954-4810-8023-e2d18a3bf08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "fp = 'embedded/prot_t5_xl_uniref50.pkl'\n",
    "pickle.dump(features, open(fp, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135871f-7723-49cc-8372-da5acb6b452d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c49020-0ca9-4f68-b058-351b2c496b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
